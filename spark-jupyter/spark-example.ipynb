{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1713d120-e949-4aff-a099-70b388627397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 02:42:13 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n",
      "25/02/05 02:42:13 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/05 02:42:14 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n",
      "25/02/05 02:42:14 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n",
      "25/02/05 02:42:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#example-1\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession with the master URL\n",
    "spark = SparkSession.builder.appName(\"example-1\").master(\"spark://spark-master:7077\").getOrCreate()\n",
    "\n",
    "# Example: Create a DataFrame\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565b4fd8-0809-4954-95e5-6b369e01fd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 02:42:49 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------+\n",
      "|   Name|Age|       City|\n",
      "+-------+---+-----------+\n",
      "|  Alice| 30|   New York|\n",
      "|    Bob| 25|Los Angeles|\n",
      "|Charlie| 35|    Chicago|\n",
      "+-------+---+-----------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#example-2 (You need to upload the information.csv) \n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress PySpark and Py4J warnings\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"sparkConf\").setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize SparkSession with the master URL\n",
    "spark = SparkSession.builder.appName(\"example-2\").master(\"spark://spark-master:7077\").getOrCreate()\n",
    "# Set Spark log level to ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# Read a CSV file into a DataFrame\n",
    "file_path = \"/spark/user/information.csv\"\n",
    "df = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffa64c5-c404-4004-a0ae-bf929f708034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|feature|label|\n",
      "+-------+-----+\n",
      "|    1.0|  2.0|\n",
      "|    2.0|  4.0|\n",
      "|    3.0|  6.0|\n",
      "|    4.0|  8.0|\n",
      "|    5.0| 10.0|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#example-3 Run spark locally\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"example-3\").master(\"local[*]\").getOrCreate()\n",
    "# Step 2: Create a synthetic dataset\n",
    "data = [\n",
    "    (1.0, 2.0),\n",
    "    (2.0, 4.0),\n",
    "    (3.0, 6.0),\n",
    "    (4.0, 8.0),\n",
    "    (5.0, 10.0)\n",
    "]\n",
    "columns = [\"feature\", \"label\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc4d3bc1-2a3a-4f46-9ed7-9e6f8a938320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.0\n",
      "+--------+-----+----------+\n",
      "|features|label|prediction|\n",
      "+--------+-----+----------+\n",
      "|   [3.0]|  6.0|       6.0|\n",
      "+--------+-----+----------+\n",
      "\n",
      "Coefficients: [2.0]\n",
      "Intercept: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Example-4\n",
    "#Build one ML model by using array data and using it the Spark cluster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"LinearRegressionExample\").master(\"spark://spark-master:7077\").getOrCreate()\n",
    "\n",
    "# Step 2: Create a synthetic dataset\n",
    "data = [\n",
    "    (1.0, 2.0),\n",
    "    (2.0, 4.0),\n",
    "    (3.0, 6.0),\n",
    "    (4.0, 8.0),\n",
    "    (5.0, 10.0)\n",
    "]\n",
    "columns = [\"feature\", \"label\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 3: Prepare the data for Linear Regression\n",
    "# Combine features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[\"feature\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 5: Create and train the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Step 6: Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Step 8: Show the predictions\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "# Step 9: Print model coefficients and intercept\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")\n",
    "\n",
    "# Step 10: Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a5034-ec13-40ee-881a-0963716050eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
